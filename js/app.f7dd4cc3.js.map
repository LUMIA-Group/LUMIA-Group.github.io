{"version":3,"file":"js/app.f7dd4cc3.js","mappings":"mEAAIA,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACE,MAAM,CAAC,GAAK,QAAQ,CAACF,EAAG,cAAcA,EAAG,gBAAgB,EACtI,EACIG,EAAkB,GCFlBN,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACI,YAAY,oBAAoB,CAACJ,EAAG,SAAS,CAACI,YAAY,kBAAkB,CAACJ,EAAG,SAAS,CAACE,MAAM,CAAC,KAAO,IAAI,CAACF,EAAG,MAAM,CAACI,YAAY,cAAcC,GAAG,CAAC,MAAQ,SAASC,GAAQ,OAAOR,EAAIS,YAAY,CAAEC,MAAO,QAAS,IAAI,CAACR,EAAG,MAAM,CAACI,YAAY,OAAOF,MAAM,CAAC,IAAMJ,EAAIW,WAAWC,MAAQ,gDAAgD,IAAM,GAAG,OAAS,MAAMV,EAAG,OAAO,CAACI,YAAY,QAAQ,CAACN,EAAIa,GAAGb,EAAIc,GAAGd,EAAIW,WAAWI,aAAab,EAAG,SAAS,CAACE,MAAM,CAAC,KAAO,KAAK,CAACF,EAAG,KAAK,CAACI,YAAY,eAAeN,EAAIgB,GAAIhB,EAAIW,WAAWM,YAAY,SAASC,GAAM,OAAOhB,EAAG,KAAK,CAACiB,IAAID,EAAKR,MAAMJ,YAAY,cAAcc,MAAM,CAAEC,OAAQrB,EAAIsB,eAAiBJ,EAAKR,OAAQH,GAAG,CAAC,MAAQ,SAASC,GAAQ,OAAOR,EAAIS,YAAYS,EAAK,IAAI,CAAClB,EAAIa,GAAG,IAAIb,EAAIc,GAAGI,EAAKK,OAAO,MAAM,IAAG,MAAM,IAAI,EAC70B,EACIlB,EAAkB,G,QCAf,MAAMM,EAAa,CAEtBI,KAAM,OAENH,KAAM,GAENK,WAAY,CAAC,CAELP,MAAO,SAEPa,MAAO,UAEX,CACIb,MAAO,WACPa,MAAO,YAEX,CACIb,MAAO,OACPa,MAAO,QAEX,CAEIb,MAAO,gBACPa,MAAO,SACPC,KAAM,QAEV,CACId,MAAO,UACPa,MAAO,aCEnB,OACAE,IAAAA,GACA,OACAd,WAAAA,EACAW,aAAA,GAEA,EACAI,OAAAA,GACA,KAAAJ,aAAA,KAAAK,OAAAZ,IACA,EACAa,QAAA,CACAnB,WAAAA,CAAAS,GACA,KAAAI,aAAAJ,EAAAR,MACA,SAAAQ,EAAAM,KACAK,OAAAC,KAAAZ,EAAAR,MAAA,UAEA,KAAAiB,OAAAZ,OAAAG,EAAAR,OACA,KAAAqB,QAAAC,KAAA,CACAjB,KAAAG,EAAAR,OAIA,ICtDsP,I,UCQlPuB,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAeA,EAAiB,QCXhC,GACAC,WAAA,CACAC,UAAAA,ICV0O,ICStO,GAAY,OACd,EACApC,EACAM,GACA,EACA,KACA,KACA,MAIF,EAAe,EAAiB,Q,UCpB5BN,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACI,YAAY,kBAAkB,CAACJ,EAAG,MAAM,CAACI,YAAY,gBAAgB,CAACJ,EAAG,UAAU,CAACI,YAAY,gBAAgB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAGb,EAAIc,GAAGd,EAAIoC,SAASrB,SAASb,EAAG,KAAK,CAACF,EAAIa,GAAG,IAAIb,EAAIc,GAAGd,EAAIoC,SAASC,MAAM,SAASnC,EAAG,UAAU,CAACI,YAAY,gBAAgB,CAACJ,EAAG,SAAS,CAACE,MAAM,CAAC,OAAS,KAAKJ,EAAIgB,GAAIhB,EAAIoC,SAASE,aAAa,SAASpB,GAAM,OAAOhB,EAAG,SAAS,CAACiB,IAAID,EAAKqB,GAAGjC,YAAY,qBAAqB,CAACJ,EAAG,UAAU,CAACI,YAAY,gBAAgB,CAACJ,EAAG,MAAM,CAACA,EAAG,MAAM,CAACE,MAAM,CAAC,IAAMc,EAAKsB,IAAI,IAAM,GAAG,OAAS,GAAG,MAAQ,eAAe,IAAG,IAAI,MAC1mB,EACInC,EAAkB,GCAf,MAAM+B,EAAW,CAEpBrB,KAAM,WAENsB,KAAM,mFAENC,YAAa,CAAC,CACVC,GAAI,EAEJC,IAAK,uBACLzB,KAAM,QACN0B,MAAO,gBCef,OACA1B,KAAA,WACAU,IAAAA,GACA,OACAW,SAAAA,EAEA,GClCwP,ICQpP,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCnB5BrC,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACI,YAAY,oBAAoB,CAACJ,EAAG,MAAM,CAACI,YAAY,kBAAkB,CAACJ,EAAG,cAAc,CAACwC,MAAM,CAAChC,MAAOV,EAAI2C,YAAaC,SAAS,SAAUC,GAAM7C,EAAI2C,YAAYE,CAAG,EAAEC,WAAW,gBAAgB,CAAC5C,EAAG,mBAAmB,CAACE,MAAM,CAAC,MAAQ,SAAS,KAAO,MAAM,CAACF,EAAG,KAAK,CAACI,YAAY,UAAUN,EAAIgB,GAAIhB,EAAI+C,WAAWC,cAAc,SAAS9B,EAAK+B,GAAO,OAAO/C,EAAG,KAAK,CAACiB,IAAI8B,EAAM3C,YAAY,UAAU,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,IAAMc,EAAKgC,IAAI,IAAM,GAAG,OAAS,GAAG,MAAQ,UAAUhD,EAAG,KAAK,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAGb,EAAIc,GAAGI,EAAKH,SAASb,EAAG,IAAI,CAACF,EAAIa,GAAGb,EAAIc,GAAGI,EAAKiC,WAAW,IAAG,KAAKjD,EAAG,mBAAmB,CAACE,MAAM,CAAC,MAAQ,SAAS,KAAO,MAAM,CAACF,EAAG,KAAK,CAACI,YAAY,UAAUN,EAAIgB,GAAIhB,EAAI+C,WAAWK,cAAc,SAASlC,EAAK+B,GAAO,OAAO/C,EAAG,KAAK,CAACiB,IAAI8B,EAAM3C,YAAY,UAAU,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,IAAMc,EAAKgC,IAAI,IAAM,GAAG,OAAS,GAAG,MAAQ,UAAUhD,EAAG,KAAK,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAGb,EAAIc,GAAGI,EAAKH,SAASb,EAAG,IAAI,CAACF,EAAIa,GAAGb,EAAIc,GAAGI,EAAKiC,WAAW,IAAG,KAAKjD,EAAG,mBAAmB,CAACE,MAAM,CAAC,MAAQ,SAAS,KAAO,MAAM,CAACF,EAAG,KAAK,CAACI,YAAY,UAAUN,EAAIgB,GAAIhB,EAAIgD,cAAc,SAAS9B,EAAK+B,GAAO,OAAO/C,EAAG,KAAK,CAACiB,IAAI8B,EAAM3C,YAAY,UAAU,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,IAAMc,EAAKgC,IAAI,IAAM,GAAG,OAAS,GAAG,MAAQ,UAAUhD,EAAG,KAAK,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAGb,EAAIc,GAAGI,EAAKH,SAASb,EAAG,IAAI,CAACF,EAAIa,GAAGb,EAAIc,GAAGI,EAAKiC,WAAW,IAAG,MAAM,IAAI,IACp3C,EACI9C,EAAkB,GCAf,MAAM0C,EAAa,CACtBC,aAAc,CAAC,CACPjC,KAAM,YACNmC,IAAK,0BACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,uBACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,0BACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,uBACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,wBACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,0BACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,uBACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,0BACLC,MAAO,OACPE,SAAU,KAGlBD,aAAc,CAAC,CACPrC,KAAM,YACNmC,IAAK,0BACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,uBACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,0BACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,uBACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,wBACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,0BACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,uBACLC,MAAO,OACPE,SAAU,IAEd,CACItC,KAAM,YACNmC,IAAK,0BACLC,MAAO,OACPE,SAAU,MC7CtB,OACA5B,IAAAA,GACA,OACAsB,WAAAA,EACAJ,YAAA,IAEA,GC3DsP,ICSlP,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCpB5B5C,EAAS,WAAkB,IAAIC,EAAIC,KAAQD,EAAIG,MAAMD,GAAG,OAAOF,EAAIsD,GAAG,EAC1E,EACIjD,EAAkB,CAAC,WAAY,IAAIL,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACI,YAAY,oBAAoB,CAACJ,EAAG,UAAU,CAACI,YAAY,eAAeF,MAAM,CAAC,GAAK,iBAAiB,CAACF,EAAG,UAAU,CAACI,YAAY,oBAAoB,CAACJ,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAA+C,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,uGAAuGX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,kFAAkFX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,oBAAoBX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,0BAA0BX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,uDAAuD,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,sDAAsD,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,6rBAA6rBX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAAqC,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,6FAA6FX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,iGAAiGX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,oBAAoBX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,kBAAkBX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,6CAA6C,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,gfAAgfX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,KAAuC,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,yFAAyFX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,gDAAgDX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,oBAAoBX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,gBAAgBX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,8CAA8C,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,oiBAAoiBX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAAkC,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,8FAA8FX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,6GAA6GX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,oBAAoBX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,iBAAiBX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,0wBAA0wBX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,IAAgD,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,iFAAiFX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,iGAAiGX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,oBAAoBX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,4BAA4BX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,wDAAwD,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,kRAAkRX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAAoC,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,mGAAmGX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,qEAAqEX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,oBAAoBX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,gBAAgBX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,gFAAgF,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,+xBAA+xBX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAAsC,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,8DAA8DX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,0CAA0CX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,kBAAkBb,EAAIa,GAAG,6BAA6BX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,iBAAiBX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,8CAA8C,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,grBAAgrBX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAA4C,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,kGAAkGX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,gBAAgBX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,iBAAiBb,EAAIa,GAAG,sEAAsEX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,gBAAgBX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,sCAAsC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,kwBAAowBX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAA2C,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,iFAAiFX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,mBAAmBX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,iBAAiBb,EAAIa,GAAG,8EAA8EX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,YAAY,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,cAAcX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,8CAA8C,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,mpBAAmpBX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAAiC,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,uEAAuEX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,kBAAkBX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,iBAAiBb,EAAIa,GAAG,wCAAwCX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,8CAA8C,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,4gCAA4gCX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAAkC,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,oDAAoDX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACJ,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,iBAAiBb,EAAIa,GAAG,iGAAiGX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACI,YAAY,QAAQ,CAACN,EAAIa,GAAG,eAAeX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,qDAAqD,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,suBAAsuBX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,MAAsC,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,8CAA8CX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACJ,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,iBAAiBb,EAAIa,GAAG,kEAAkEX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACI,YAAY,QAAQ,CAACN,EAAIa,GAAG,sBAAsBX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,yCAAyC,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,4CAA4C,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,26BAA26BX,EAAG,MAAM,CAACI,YAAY,OAAO,CAACJ,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,MAAM,CAACE,MAAM,CAAC,MAAQ,OAAO,IAAMmD,EAAQ,KAAwC,IAAM,GAAG,OAAS,QAAQrD,EAAG,MAAM,CAACI,YAAY,sBAAsB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAG,8DAA8DX,EAAG,IAAI,CAACI,YAAY,UAAU,CAACN,EAAIa,GAAG,iBAAiBX,EAAG,OAAO,CAACsD,YAAY,CAAC,cAAc,QAAQ,CAACxD,EAAIa,GAAG,iBAAiBb,EAAIa,GAAG,6CAA6CX,EAAG,MAAM,CAACI,YAAY,aAAa,CAACJ,EAAG,OAAO,CAACI,YAAY,QAAQ,CAACN,EAAIa,GAAG,uFAAuFX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,mEAAmE,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,WAAWX,EAAG,OAAO,CAACF,EAAIa,GAAG,SAASX,EAAG,IAAI,CAACE,MAAM,CAAC,KAAO,4CAA4C,CAACF,EAAG,OAAO,CAACF,EAAIa,GAAG,eAAeX,EAAG,UAAU,CAACF,EAAIa,GAAG,y0CAC/vnB,GCDO,MAAM4C,EAAe,CAExBC,aAAc,CAEV,CAEI3C,KAAM,4EAENyB,IAAK,uCAELmB,KAAM,CAAC,CACC5C,KAAM,YAEV,CACIA,KAAM,MACN6C,KAAM,SAEV,CACI7C,KAAM,SACN6C,KAAM,SAEV,CACI7C,KAAM,QACN6C,KAAM,UAGdC,OAAQ,mGAERC,SAAU,2oBAEd,CACI/C,KAAM,4EACNyB,IAAK,uCACLmB,KAAM,CAAC,CACC5C,KAAM,YAEV,CACIA,KAAM,MACN6C,KAAM,SAEV,CACI7C,KAAM,SACN6C,KAAM,SAEV,CACI7C,KAAM,QACN6C,KAAM,UAGdC,OAAQ,mGACRC,SAAU,2oBAEd,CACI/C,KAAM,4EACNyB,IAAK,uCACLmB,KAAM,CAAC,CACC5C,KAAM,YAEV,CACIA,KAAM,MACN6C,KAAM,SAEV,CACI7C,KAAM,SACN6C,KAAM,SAEV,CACI7C,KAAM,QACN6C,KAAM,UAGdC,OAAQ,mGACRC,SAAU,2oBAEd,CACI/C,KAAM,4EACNyB,IAAK,uCACLmB,KAAM,CAAC,CACC5C,KAAM,YAEV,CACIA,KAAM,MACN6C,KAAM,SAEV,CACI7C,KAAM,SACN6C,KAAM,SAEV,CACI7C,KAAM,QACN6C,KAAM,UAGdC,OAAQ,mGACRC,SAAU,2oBAEd,CACI/C,KAAM,4EACNyB,IAAK,uCACLmB,KAAM,CAAC,CACC5C,KAAM,YAEV,CACIA,KAAM,MACN6C,KAAM,SAEV,CACI7C,KAAM,SACN6C,KAAM,SAEV,CACI7C,KAAM,QACN6C,KAAM,UAGdC,OAAQ,mGACRC,SAAU,6oBCwgBtB,OACArC,IAAAA,GACA,OACAgC,aAAAA,EAEA,GCnoBwP,ICQpP,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCnB5B1D,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACI,YAAY,kBAAkB,CAACJ,EAAG,MAAM,CAACI,YAAY,gBAAgB,CAACJ,EAAG,KAAKF,EAAIgB,GAAIhB,EAAI+D,SAASC,UAAU,SAAS9C,EAAK+B,GAAO,OAAO/C,EAAG,KAAK,CAACiB,IAAI8B,GAAO,CAACjD,EAAIa,GAAG,IAAIb,EAAIc,GAAGI,GAAM,MAAM,IAAG,MAC5Q,EACIb,EAAkB,GCAf,MAAM0D,EAAW,CACpBC,SAAU,CAEN,iDACA,iDACA,iDACA,mDCKR,OACAvC,IAAAA,GACA,OACAsC,SAAAA,EAEA,GClBoP,ICQhP,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCnB5BhE,EAAS,WAAkB,IAAIC,EAAIC,KAAKC,EAAGF,EAAIG,MAAMD,GAAG,OAAOA,EAAG,MAAM,CAACI,YAAY,qBAAqB,CAACJ,EAAG,MAAM,CAACI,YAAY,mBAAmBN,EAAIgB,GAAIhB,EAAIiE,YAAYC,aAAa,SAAShD,EAAK+B,GAAO,OAAO/C,EAAG,UAAU,CAACiB,IAAI8B,EAAM3C,YAAY,gBAAgB,CAACJ,EAAG,KAAK,CAACF,EAAIa,GAAGb,EAAIc,GAAGI,EAAKiD,WAAWnE,EAAIgB,GAAIE,EAAKyC,MAAM,SAASS,EAAEC,GAAQ,OAAOnE,EAAG,IAAI,CAACiB,IAAIkD,EAAS,KAAK,CAACrE,EAAIa,GAAGb,EAAIc,GAAGsD,KAAK,KAAI,EAAE,IAAG,IACzZ,EACI/D,EAAkB,GCAf,MAAM4D,EAAc,CACvBC,YAAa,CAET,CACIC,OAAQ,WACRR,KAAM,CAEF,iBAEA,kTAIR,CACIQ,OAAQ,WACRR,KAAM,CAAC,mBAEX,CACIQ,OAAQ,WACRR,KAAM,CAAC,qBCLnB,OACAlC,IAAAA,GACA,OACAwC,YAAAA,EAEA,GCrBuP,ICQnP,GAAY,OACd,EACA,EACA,GACA,EACA,KACA,WACA,MAIF,EAAe,EAAiB,QCXhCK,EAAAA,WAAIC,IAAIC,EAAAA,IAER,MAAMC,GAAS,CAAC,CACRC,KAAM,IACN3D,KAAM,OACNkB,UAAW0C,GACZ,CACCD,KAAM,UACN3D,KAAM,SACNkB,UAAW2C,GAEf,CACIF,KAAM,YACN3D,KAAM,WACNkB,UAAW4C,GAEf,CACIH,KAAM,QACN3D,KAAM,OACNkB,UAAW6C,GAEf,CACIJ,KAAM,WACN3D,KAAM,UACNkB,UAAW8C,IAIbC,GAAS,IAAIR,EAAAA,GAAU,CAEzBS,KAAMC,IACNT,YAGJ,U,UCvCAH,EAAAA,WAAIC,IAAIY,GAAAA,IAER,WAAmBA,GAAAA,GAAAA,MAAW,CAC5BC,MAAO,CAAC,EACRC,QAAS,CAAC,EACVC,UAAW,CAAC,EACZC,QAAS,CAAC,EACVC,QAAS,CAAC,I,sBCFZlB,EAAAA,WAAIC,IAAIkB,MACRnB,EAAAA,WAAIoB,OAAOC,eAAgB,EAE3B,IAAIrB,EAAAA,WAAI,CACJU,OAAM,GACNY,MAAK,GACL7F,OAAS8F,GAAMA,EAAEC,KAClBC,OAAO,O,61BCdNC,EAA2B,CAAC,EAGhC,SAASC,EAAoBC,GAE5B,IAAIC,EAAeH,EAAyBE,GAC5C,QAAqBE,IAAjBD,EACH,OAAOA,EAAaE,QAGrB,IAAIC,EAASN,EAAyBE,GAAY,CACjD3D,GAAI2D,EACJK,QAAQ,EACRF,QAAS,CAAC,GAUX,OANAG,EAAoBN,GAAUO,KAAKH,EAAOD,QAASC,EAAQA,EAAOD,QAASJ,GAG3EK,EAAOC,QAAS,EAGTD,EAAOD,OACf,CAGAJ,EAAoBS,EAAIF,E,WC5BxBP,EAAoBU,KAAO,CAAC,C,eCA5B,IAAIC,EAAW,GACfX,EAAoBY,EAAI,SAASC,EAAQC,EAAUC,EAAIC,GACtD,IAAGF,EAAH,CAMA,IAAIG,EAAeC,IACnB,IAASC,EAAI,EAAGA,EAAIR,EAASS,OAAQD,IAAK,CACrCL,EAAWH,EAASQ,GAAG,GACvBJ,EAAKJ,EAASQ,GAAG,GACjBH,EAAWL,EAASQ,GAAG,GAE3B,IAJA,IAGIE,GAAY,EACPC,EAAI,EAAGA,EAAIR,EAASM,OAAQE,MACpB,EAAXN,GAAsBC,GAAgBD,IAAaO,OAAOC,KAAKxB,EAAoBY,GAAGa,OAAM,SAASvG,GAAO,OAAO8E,EAAoBY,EAAE1F,GAAK4F,EAASQ,GAAK,IAChKR,EAASY,OAAOJ,IAAK,IAErBD,GAAY,EACTL,EAAWC,IAAcA,EAAeD,IAG7C,GAAGK,EAAW,CACbV,EAASe,OAAOP,IAAK,GACrB,IAAIQ,EAAIZ,SACEZ,IAANwB,IAAiBd,EAASc,EAC/B,CACD,CACA,OAAOd,CArBP,CAJCG,EAAWA,GAAY,EACvB,IAAI,IAAIG,EAAIR,EAASS,OAAQD,EAAI,GAAKR,EAASQ,EAAI,GAAG,GAAKH,EAAUG,IAAKR,EAASQ,GAAKR,EAASQ,EAAI,GACrGR,EAASQ,GAAK,CAACL,EAAUC,EAAIC,EAwB/B,C,eC5BAhB,EAAoB4B,EAAI,SAASvB,GAChC,IAAIwB,EAASxB,GAAUA,EAAOyB,WAC7B,WAAa,OAAOzB,EAAO,UAAY,EACvC,WAAa,OAAOA,CAAQ,EAE7B,OADAL,EAAoB+B,EAAEF,EAAQ,CAAEG,EAAGH,IAC5BA,CACR,C,eCNA7B,EAAoB+B,EAAI,SAAS3B,EAAS6B,GACzC,IAAI,IAAI/G,KAAO+G,EACXjC,EAAoBkC,EAAED,EAAY/G,KAAS8E,EAAoBkC,EAAE9B,EAASlF,IAC5EqG,OAAOY,eAAe/B,EAASlF,EAAK,CAAEkH,YAAY,EAAMC,IAAKJ,EAAW/G,IAG3E,C,eCPA8E,EAAoBsC,EAAI,WACvB,GAA0B,kBAAfC,WAAyB,OAAOA,WAC3C,IACC,OAAOvI,MAAQ,IAAIwI,SAAS,cAAb,EAChB,CAAE,MAAOC,GACR,GAAsB,kBAAX7G,OAAqB,OAAOA,MACxC,CACA,CAPuB,E,eCAxBoE,EAAoBkC,EAAI,SAASQ,EAAKC,GAAQ,OAAOpB,OAAOqB,UAAUC,eAAerC,KAAKkC,EAAKC,EAAO,C,eCCtG3C,EAAoB2B,EAAI,SAASvB,GACX,qBAAX0C,QAA0BA,OAAOC,aAC1CxB,OAAOY,eAAe/B,EAAS0C,OAAOC,YAAa,CAAEtI,MAAO,WAE7D8G,OAAOY,eAAe/B,EAAS,aAAc,CAAE3F,OAAO,GACvD,C,eCNAuF,EAAoBgD,IAAM,SAAS3C,GAGlC,OAFAA,EAAO4C,MAAQ,GACV5C,EAAO6C,WAAU7C,EAAO6C,SAAW,IACjC7C,CACR,C,eCJAL,EAAoBmD,EAAI,G,eCKxB,IAAIC,EAAkB,CACrB,IAAK,GAaNpD,EAAoBY,EAAEU,EAAI,SAAS+B,GAAW,OAAoC,IAA7BD,EAAgBC,EAAgB,EAGrF,IAAIC,EAAuB,SAASC,EAA4B/H,GAC/D,IAKIyE,EAAUoD,EALVvC,EAAWtF,EAAK,GAChBgI,EAAchI,EAAK,GACnBiI,EAAUjI,EAAK,GAGI2F,EAAI,EAC3B,GAAGL,EAAS4C,MAAK,SAASpH,GAAM,OAA+B,IAAxB8G,EAAgB9G,EAAW,IAAI,CACrE,IAAI2D,KAAYuD,EACZxD,EAAoBkC,EAAEsB,EAAavD,KACrCD,EAAoBS,EAAER,GAAYuD,EAAYvD,IAGhD,GAAGwD,EAAS,IAAI5C,EAAS4C,EAAQzD,EAClC,CAEA,IADGuD,GAA4BA,EAA2B/H,GACrD2F,EAAIL,EAASM,OAAQD,IACzBkC,EAAUvC,EAASK,GAChBnB,EAAoBkC,EAAEkB,EAAiBC,IAAYD,EAAgBC,IACrED,EAAgBC,GAAS,KAE1BD,EAAgBC,GAAW,EAE5B,OAAOrD,EAAoBY,EAAEC,EAC9B,EAEI8C,EAAqBC,KAAK,2BAA6BA,KAAK,4BAA8B,GAC9FD,EAAmBE,QAAQP,EAAqBQ,KAAK,KAAM,IAC3DH,EAAmB5H,KAAOuH,EAAqBQ,KAAK,KAAMH,EAAmB5H,KAAK+H,KAAKH,G,IC/CvF,IAAII,EAAsB/D,EAAoBY,OAAET,EAAW,CAAC,MAAM,WAAa,OAAOH,EAAoB,KAAO,IACjH+D,EAAsB/D,EAAoBY,EAAEmD,E","sources":["webpack://LUMIA-Group/./src/App.vue","webpack://LUMIA-Group/./src/components/Header.vue","webpack://LUMIA-Group/./src/data/header.js","webpack://LUMIA-Group/src/components/Header.vue","webpack://LUMIA-Group/./src/components/Header.vue?2279","webpack://LUMIA-Group/./src/components/Header.vue?03e2","webpack://LUMIA-Group/src/App.vue","webpack://LUMIA-Group/./src/App.vue?51dd","webpack://LUMIA-Group/./src/App.vue?0e40","webpack://LUMIA-Group/./src/views/HomeView.vue","webpack://LUMIA-Group/./src/data/home.js","webpack://LUMIA-Group/src/views/HomeView.vue","webpack://LUMIA-Group/./src/views/HomeView.vue?2a07","webpack://LUMIA-Group/./src/views/HomeView.vue?aae2","webpack://LUMIA-Group/./src/views/People.vue","webpack://LUMIA-Group/./src/data/people.js","webpack://LUMIA-Group/src/views/People.vue","webpack://LUMIA-Group/./src/views/People.vue?abab","webpack://LUMIA-Group/./src/views/People.vue?340c","webpack://LUMIA-Group/./src/views/Research.vue","webpack://LUMIA-Group/./src/data/research.js","webpack://LUMIA-Group/src/views/Research.vue","webpack://LUMIA-Group/./src/views/Research.vue?f954","webpack://LUMIA-Group/./src/views/Research.vue?6c55","webpack://LUMIA-Group/./src/views/News.vue","webpack://LUMIA-Group/./src/data/news.js","webpack://LUMIA-Group/src/views/News.vue","webpack://LUMIA-Group/./src/views/News.vue?1e70","webpack://LUMIA-Group/./src/views/News.vue?c751","webpack://LUMIA-Group/./src/views/Contact.vue","webpack://LUMIA-Group/./src/data/contact.js","webpack://LUMIA-Group/src/views/Contact.vue","webpack://LUMIA-Group/./src/views/Contact.vue?3aa6","webpack://LUMIA-Group/./src/views/Contact.vue?f906","webpack://LUMIA-Group/./src/router/index.js","webpack://LUMIA-Group/./src/store/index.js","webpack://LUMIA-Group/./src/main.js","webpack://LUMIA-Group/webpack/bootstrap","webpack://LUMIA-Group/webpack/runtime/amd options","webpack://LUMIA-Group/webpack/runtime/chunk loaded","webpack://LUMIA-Group/webpack/runtime/compat get default export","webpack://LUMIA-Group/webpack/runtime/define property getters","webpack://LUMIA-Group/webpack/runtime/global","webpack://LUMIA-Group/webpack/runtime/hasOwnProperty shorthand","webpack://LUMIA-Group/webpack/runtime/make namespace object","webpack://LUMIA-Group/webpack/runtime/node module decorator","webpack://LUMIA-Group/webpack/runtime/publicPath","webpack://LUMIA-Group/webpack/runtime/jsonp chunk loading","webpack://LUMIA-Group/webpack/startup"],"sourcesContent":["var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{attrs:{\"id\":\"app\"}},[_c('app-header'),_c('router-view')],1)\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"header-container\"},[_c('el-row',{staticClass:\"header-content\"},[_c('el-col',{attrs:{\"span\":8}},[_c('div',{staticClass:\"header-logo\",on:{\"click\":function($event){return _vm.clickHeader({ value: 'home' })}}},[_c('img',{staticClass:\"logo\",attrs:{\"src\":_vm.headerData.logo || 'https://vi.sjtu.edu.cn/img/brand-logo-s-w.png',\"alt\":\"\",\"srcset\":\"\"}}),_c('span',{staticClass:\"name\"},[_vm._v(_vm._s(_vm.headerData.name))])])]),_c('el-col',{attrs:{\"span\":16}},[_c('ul',{staticClass:\"header-list\"},_vm._l((_vm.headerData.headerList),function(item){return _c('li',{key:item.value,staticClass:\"header-item\",class:{ active: _vm.activeHeader === item.value },on:{\"click\":function($event){return _vm.clickHeader(item)}}},[_vm._v(\" \"+_vm._s(item.label)+\" \")])}),0)])],1)],1)\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","// 网站header相关数据\n// 按规则修改和添加内容即可，请不要更改数据结构以及变量命名\nexport const headerData = {\n    // 网站名称\n    name: 'Name',\n    // logo图片路径\n    logo: '',\n    // header中的每一个tab\n    headerList: [{\n            // 对应的路由名称，需要与router文件夹中的路由对应，详情见readme\n            value: \"people\",\n            // 显示在网页header中的名字\n            label: \"People\",\n        },\n        {\n            value: \"research\",\n            label: \"Research\",\n        },\n        {\n            value: \"news\",\n            label: \"News\",\n        },\n        {\n            // 若需要链接到外部网站，需要加一个type: \"link\"，则在value中填入链接的URL\n            value: \"http://github\",\n            label: \"Github\",\n            type: \"link\",\n        },\n        {\n            value: \"contact\",\n            label: \"Contact\",\n        },\n    ],\n}","<template>\n  <div class=\"header-container\">\n    <el-row class=\"header-content\">\n      <el-col :span=\"8\">\n        <div class=\"header-logo\" @click=\"clickHeader({ value: 'home' })\">\n          <img\n            class=\"logo\"\n            :src=\"headerData.logo || 'https://vi.sjtu.edu.cn/img/brand-logo-s-w.png'\"\n            alt=\"\"\n            srcset=\"\"\n          />\n          <span class=\"name\">{{ headerData.name }}</span>\n        </div>\n      </el-col>\n      <el-col :span=\"16\">\n        <ul class=\"header-list\">\n          <li\n            class=\"header-item\"\n            :class=\"{ active: activeHeader === item.value }\"\n            v-for=\"item in headerData.headerList\"\n            :key=\"item.value\"\n            @click=\"clickHeader(item)\"\n          >\n            {{ item.label }}\n          </li>\n        </ul>\n      </el-col>\n    </el-row>\n  </div>\n</template>\n<script>\nimport { headerData } from '@/data/header'\nexport default {\n  data() {\n    return {\n        headerData: headerData,\n      activeHeader: \"\",\n    };\n  },\n  mounted() {\n      this.activeHeader = this.$route.name\n  },\n  methods: {\n    clickHeader(item) {\n      this.activeHeader = item.value;\n      if (item.type === \"link\") {\n        window.open(item.value, \"_blank\");\n      } else {\n        if (this.$route.name !== item.value) {\n          this.$router.push({\n            name: item.value,\n          });\n        }\n      }\n    },\n  },\n};\n</script>\n<style lang=\"less\" scoped>\n.header-container {\n  height: 78px;\n  line-height: 78px;\n  background: #004d4c;\n  color: #fff;\n  .header-content {\n    max-width: 1280px;\n    margin: auto;\n  }\n  .header-logo {\n    display: flex;\n    align-items: center;\n    cursor: pointer;\n  }\n  .logo {\n    width: 50px;\n    margin-right: 12px;\n  }\n  .name {\n    font-weight: 500;\n    font-size: 20px;\n  }\n  .header-list {\n    display: flex;\n    justify-content: flex-end;\n    .header-item {\n      padding: 0 24px;\n      font-weight: 600;\n      font-size: 18px;\n      cursor: pointer;\n      &.active {\n        box-shadow: inset 0 -4px #fff;\n      }\n      &:hover {\n        box-shadow: inset 0 -4px #fff;\n      }\n    }\n  }\n}\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Header.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Header.vue?vue&type=template&id=3dfd6b96&scoped=true&\"\nimport script from \"./Header.vue?vue&type=script&lang=js&\"\nexport * from \"./Header.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Header.vue?vue&type=style&index=0&id=3dfd6b96&prod&lang=less&scoped=true&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"3dfd6b96\",\n  null\n  \n)\n\nexport default component.exports","<template>\n  <div id=\"app\">\n    <app-header/>\n    <router-view />\n  </div>\n</template>\n<script>\nimport appHeader from '@/components/Header.vue'\nexport default {\n  components: {\n    appHeader\n  }\n}\n</script>\n<style>\n@import url('./reset.css');\n</style>\n<style lang=\"less\">\n#app {\n  font-family: Avenir, Helvetica, Arial, sans-serif;\n  -webkit-font-smoothing: antialiased;\n  -moz-osx-font-smoothing: grayscale;\n  text-align: center;\n  color: #2c3e50;\n}\n\nnav {\n  padding: 30px;\n\n  a {\n    font-weight: bold;\n    color: #2c3e50;\n\n    &.router-link-exact-active {\n      color: #42b983;\n    }\n  }\n}\n</style>\n","import mod from \"-!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../node_modules/thread-loader/dist/cjs.js!../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./App.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./App.vue?vue&type=template&id=4a965dfc&\"\nimport script from \"./App.vue?vue&type=script&lang=js&\"\nexport * from \"./App.vue?vue&type=script&lang=js&\"\nimport style0 from \"./App.vue?vue&type=style&index=0&id=4a965dfc&prod&lang=css&\"\nimport style1 from \"./App.vue?vue&type=style&index=1&id=4a965dfc&prod&lang=less&\"\n\n\n/* normalize component */\nimport normalizer from \"!../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  null,\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"home-container\"},[_c('div',{staticClass:\"home-content\"},[_c('section',{staticClass:\"introduction\"},[_c('h1',[_vm._v(_vm._s(_vm.homeData.name))]),_c('h2',[_vm._v(\" \"+_vm._s(_vm.homeData.desc)+\" \")])]),_c('section',{staticClass:\"project-list\"},[_c('el-row',{attrs:{\"gutter\":32}},_vm._l((_vm.homeData.projectList),function(item){return _c('el-col',{key:item.id,staticClass:\"peoject-container\"},[_c('section',{staticClass:\"project-item\"},[_c('div',[_c('img',{attrs:{\"src\":item.img,\"alt\":\"\",\"srcset\":\"\",\"width\":\"100%\"}})])])])}),1)],1)])])\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","// 主页相关数据\n// 按规则修改和添加内容即可，请不要更改数据结构以及变量命名\nexport const homeData = {\n    // 实验室网站名称\n    name: 'New Name',\n    // 实验室介绍\n    desc: 'Our latest web design tips, tricks, insights, and resources, hot offthe presses.',\n    // 相关介绍的图片及文字\n    projectList: [{\n        id: 1,\n        // 图片路径\n        img: './static/testimg.png',\n        name: 'Name1',\n        intro: 'This is ……'\n    }, ],\n}","<template>\n  <div class=\"home-container\">\n    <div class=\"home-content\">\n      <section class=\"introduction\">\n        <h1>{{ homeData.name }}</h1>\n        <h2>\n          {{ homeData.desc }}\n        </h2>\n      </section>\n      <section class=\"project-list\">\n        <el-row :gutter=\"32\">\n          <el-col class=\"peoject-container\" v-for=\"item in homeData.projectList\" :key=\"item.id\">\n            <section class=\"project-item\">\n              <div>\n                <img :src=\"item.img\" alt=\"\" srcset=\"\" width=\"100%\">\n              </div>\n              <!-- <h3>{{ item.name }}</h3>\n              <p>{{ item.intro }}</p> -->\n            </section>\n          </el-col>\n        </el-row>\n      </section>\n    </div>\n  </div>\n</template>\n\n<script>\nimport { homeData } from '@/data/home'\nexport default {\n  name: \"homepage\",\n  data() {\n    return {\n      homeData: homeData\n    }\n  }\n};\n</script>\n\n<style lang=\"less\" scoped>\n.home-content {\n  max-width: 1280px;\n  margin: auto;\n  padding-bottom: 48px;\n}\n.introduction {\n  padding-top: 120px;\n  // padding-bottom: 120px;\n  padding-bottom: 36px;\n  text-align: left;\n  width: 60%;\n  h1 {\n    font-size: 68px;\n    font-weight: 700;\n    margin-bottom: 32px;\n  }\n  h2 {\n    font-size: 26px;\n    line-height: 40px;\n  }\n}\n.project-item {\n  // padding: 18px;\n  // text-align: left;\n  // &:hover {\n  //   border: 1px solid #000;\n  //   cursor: pointer;\n  // }\n  // h3 {\n  //   font-size: 22px;\n  //   font-weight: 600;\n  //   margin: 24px 0;\n  // }\n}\n.peoject-container {\n  // margin-bottom: 36px;\n  // &:nth-child(2n - 1) {\n  //   padding-top: 64px;\n  // }\n}\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./HomeView.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./HomeView.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./HomeView.vue?vue&type=template&id=3f67d61b&scoped=true&\"\nimport script from \"./HomeView.vue?vue&type=script&lang=js&\"\nexport * from \"./HomeView.vue?vue&type=script&lang=js&\"\nimport style0 from \"./HomeView.vue?vue&type=style&index=0&id=3f67d61b&prod&lang=less&scoped=true&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"3f67d61b\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"people-container\"},[_c('div',{staticClass:\"people-content\"},[_c('el-collapse',{model:{value:(_vm.activeNames),callback:function ($$v) {_vm.activeNames=$$v},expression:\"activeNames\"}},[_c('el-collapse-item',{attrs:{\"title\":\"Name 1\",\"name\":\"1\"}},[_c('ul',{staticClass:\"s-list\"},_vm._l((_vm.peopleData.studentsList),function(item,index){return _c('li',{key:index,staticClass:\"s-item\"},[_c('img',{attrs:{\"src\":item.pic,\"alt\":\"\",\"srcset\":\"\",\"width\":\"100%\"}}),_c('h3',{staticClass:\"s-name\"},[_vm._v(_vm._s(item.name))]),_c('p',[_vm._v(_vm._s(item.grade))])])}),0)]),_c('el-collapse-item',{attrs:{\"title\":\"Name 2\",\"name\":\"2\"}},[_c('ul',{staticClass:\"s-list\"},_vm._l((_vm.peopleData.teachersList),function(item,index){return _c('li',{key:index,staticClass:\"s-item\"},[_c('img',{attrs:{\"src\":item.pic,\"alt\":\"\",\"srcset\":\"\",\"width\":\"100%\"}}),_c('h3',{staticClass:\"s-name\"},[_vm._v(_vm._s(item.name))]),_c('p',[_vm._v(_vm._s(item.grade))])])}),0)]),_c('el-collapse-item',{attrs:{\"title\":\"Name 3\",\"name\":\"3\"}},[_c('ul',{staticClass:\"s-list\"},_vm._l((_vm.studentsList),function(item,index){return _c('li',{key:index,staticClass:\"s-item\"},[_c('img',{attrs:{\"src\":item.pic,\"alt\":\"\",\"srcset\":\"\",\"width\":\"100%\"}}),_c('h3',{staticClass:\"s-name\"},[_vm._v(_vm._s(item.name))]),_c('p',[_vm._v(_vm._s(item.grade))])])}),0)])],1)],1)])\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","// People页面相关数据\n// 按规则修改和添加内容即可，请不要更改数据结构以及变量命名\nexport const peopleData = {\n    studentsList: [{\n            name: \"A student\",\n            pic: \"./static/testpeople.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testimg.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testpeople.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testimg.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testimg2.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testpeople.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testimg.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testpeople.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n    ],\n    teachersList: [{\n            name: \"A student\",\n            pic: \"./static/testpeople.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testimg.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testpeople.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testimg.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testimg2.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testpeople.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testimg.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n        {\n            name: \"A student\",\n            pic: \"./static/testpeople.png\",\n            grade: \"XXXX\",\n            homepage: \"\",\n        },\n    ],\n}","<template>\n  <div class=\"people-container\">\n    <div class=\"people-content\">\n      <el-collapse v-model=\"activeNames\">\n        <!-- 在这里修改类目名称，如将“Name 1”改为“students” -->\n        <el-collapse-item title=\"Name 1\" name=\"1\">\n          <ul class=\"s-list\">\n            <li\n              class=\"s-item\"\n              v-for=\"(item, index) in peopleData.studentsList\"\n              :key=\"index\"\n            >\n              <img :src=\"item.pic\" alt=\"\" srcset=\"\" width=\"100%\"/>\n              <h3 class=\"s-name\">{{ item.name }}</h3>\n              <p>{{ item.grade }}</p>\n            </li>\n          </ul>\n        </el-collapse-item>\n         <!-- 在这里修改类目名称，如将“Name 1”改为“students” -->\n        <el-collapse-item title=\"Name 2\" name=\"2\">\n          <ul class=\"s-list\">\n            <li\n              class=\"s-item\"\n              v-for=\"(item, index) in peopleData.teachersList\"\n              :key=\"index\"\n            >\n              <img :src=\"item.pic\" alt=\"\" srcset=\"\" width=\"100%\"/>\n              <h3 class=\"s-name\">{{ item.name }}</h3>\n              <p>{{ item.grade }}</p>\n            </li>\n          </ul>\n        </el-collapse-item>\n         <!-- 在这里修改类目名称，如将“Name 1”改为“students” -->\n        <el-collapse-item title=\"Name 3\" name=\"3\">\n          <ul class=\"s-list\">\n            <li\n              class=\"s-item\"\n              v-for=\"(item, index) in studentsList\"\n              :key=\"index\"\n            >\n              <img :src=\"item.pic\" alt=\"\" srcset=\"\" width=\"100%\"/>\n              <h3 class=\"s-name\">{{ item.name }}</h3>\n              <p>{{ item.grade }}</p>\n            </li>\n          </ul>\n        </el-collapse-item>\n         <!-- 在这里新建一个类目，格式同上 -->\n      </el-collapse>\n    </div>\n  </div>\n</template>\n<script>\nimport { peopleData } from '@/data/people'\nexport default {\n  data() {\n    return {\n      peopleData: peopleData,\n      activeNames: \"1\",\n    };\n  },\n};\n</script>\n<style lang=\"less\" scoped>\n.people-container {\n  max-width: 1280px;\n  margin: auto;\n  padding-top: 80px;\n  .s-list {\n    width: 100%; // 默认宽度\n    margin: 20px auto; // 剧中\n    columns: 4; // 默认列数\n    column-gap: 30px; // 列间距\n    .s-item {\n      width: 100%;\n      break-inside: avoid;\n      margin-bottom: 30px;\n    }\n    .s-name {\n        font-size: 18px;\n        font-weight: 600;\n    }\n  }\n}\n</style>\n<style lang=\"less\">\n.people-container {\n  .el-collapse-item__header {\n    color: #004d4c;\n    font-size: 18px;\n    font-weight: 600;\n  }\n}\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./People.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./People.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./People.vue?vue&type=template&id=0d8ec9a6&scoped=true&\"\nimport script from \"./People.vue?vue&type=script&lang=js&\"\nexport * from \"./People.vue?vue&type=script&lang=js&\"\nimport style0 from \"./People.vue?vue&type=style&index=0&id=0d8ec9a6&prod&lang=less&scoped=true&\"\nimport style1 from \"./People.vue?vue&type=style&index=1&id=0d8ec9a6&prod&lang=less&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"0d8ec9a6\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _vm._m(0)\n}\nvar staticRenderFns = [function (){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"research-container\"},[_c('div',{staticClass:\"research-content\"},[_c('section',{staticClass:\"section-item\",attrs:{\"id\":\"publications\"}},[_c('section',{staticClass:\"research-section\"},[_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/fouriertransformer.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\" Fourier Transformer: Fast Long Range Modeling by Removing Sequence Redundancy with FFT Operator \")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang, Jingwen Leng, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin#\")])]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\"ACL 2023 (Findings) \")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://aclanthology.org/2023.findings-acl.570.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/LUMIA-Group/FourierTransformer\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" We propose Fourier Transformer, a simple yet effective approach by layer-wise progressively removing sequence redundancies in hidden states using the ready-made Fast Fourier Transform (FFT) operator to perform Discrete Cosine Transformation (DCT). Fourier Transformer is able to significantly reduce computational costs while retaining the ability to inherit from various large pretrained models. SOTA performances among all transformer-based models on the LRA benchmark with significant improvement in both speed and space. For generative seq-to-seq tasks including CNN/DailyMail and ELI5, by inheriting the BART weights our model outperforms the standard BART and other efficient models. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/lotinsts.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\" Text Classification In The Wild: A Large-Scale Long-Tailed Name Normalization Dataset \")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Jiexing Qi, Shuhao Li, Zhixin Guo, Yusheng Huang, Chenghu Zhou, Weinan Zhang, Xinbing Wang, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin#\")])]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\"ICASSP 2023 \")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/2302.09509.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/LUMIA-Group/LoT-insts\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" In this work, we first collect a large-scale institution name normalization dataset LoT-insts, which contains over 25k classes that exhibit a naturally long-tailed distribution. In order to isolate the few-shot and zero-shot learning scenarios from the massive many-shot classes, we construct our test set from four different subsets: many-, medium-, and few-shot sets, as well as a zero-shot open set. We believe it provides an important and different scenario to study this problem. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/orderedgnn.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\" Ordered GNN: Ordering Message Passing to Deal with Heterophily and Over-smoothing \")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Yunchong Song, Chenghu Zhou, Xinbing Wang, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin#\")])]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\"ICLR 2023 \")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/2302.01524.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/LUMIA-Group/OrderedGNN\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" In this work, we propose to tackle both heterophily and over-smoothing problems by an ordered message passing mecanism, with specific blocks of neurons in a node embedding targeted for messages passed from neighboring nodes that are located within specific hops. This is achieved by aligning the hierarchy of the rooted-tree of a central node with the ordered neurons in its node representation. SOTA performance in both homophily and heterophily settings without any targeted design, robust to a wide number of layers, and explainable. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/rasat.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\" RASAT: Integrating Relational Structures into Pretrained Seq2Seq Model for Text-to-SQL \")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng, Chenghu Zhou, Xinbing Wang, Quanshi Zhang, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin#\")])]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\"EMNLP 2022 \")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/2205.06983.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/LUMIA-Group/rasat\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" Relational structures such as schema linking and schema encoding have been validated as a key component to qualitatively translating natural language into SQL queries. We propose RASAT: a Transformer seq2seq architecture augmented with relation-aware self-attention that could leverage a variety of relational structures while still being able to inherit the pretrained parameters from the T5 model effectively. Our model can incorporate almost all types of existing relations in the literature. Experimental results on three widely used text-to-SQL datasets, covering both single-turn and multi-turn scenarios, have shown that RASAT could achieve state-of-the-art results across all three benchmarks (75.5% EX on Spider, 52.6% IEX on SParC, and 37.4% IEX on CoSQL). \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/distancetransformer.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\" Syntax-guided Localized Self-attention by Constituency Syntactic Distance \")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Shengyuan Hou*, Jushi Kai*, Haotian Xue*, Bingyu Zhu, Bo Yuan, Longtao Huang, Xinbing Wang, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin#\")])]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\"EMNLP 2022 (Findings) \")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/2210.11759.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/LUMIA-Group/distance_transformer\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" We propose a syntax-guided localized self-attention for Transformer that allows directly incorporating grammar structures from an external constituency parser. It prohibits the attention mechanism from overweight the grammatically distant tokens over close ones. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/avsrpan.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\" Leveraging Unimodal Self-Supervised Learning for Multimodal Audio-Visual Speech Recognition \")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Xichen Pan, Peiyu Chen, Yichen Gong, Helong Zhou, Xinbing Wang, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin#\")])]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\" ACL 2022 \")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/2203.07996.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/LUMIA-Group/Leveraging-Self-Supervised-Learning-for-AVSR\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" In this work, we successfully leverage unimodal self-supervised learning to promote the multimodal AVSR. In particular, audio and visual front-ends are trained on large-scale unimodal datasets, and then we integrate components of both front-ends into a larger multimodal framework that learns to recognize parallel audio-visual data into characters through a combination of CTC and seq2seq decoding. We show that both components inherited from unimodal self-supervised learning cooperate well, resulting in the multimodal framework yielding competitive results through fine-tuning. Even without an external language model, our proposed model raises the SOTA performances on the widely accepted Lip Reading Sentences 2 (LRS2) dataset by a large margin, with a relative improvement of 30%. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/blockskim.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\"Block-Skim: Efficient Question Answering for Transformer\")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Yue Guan, Zhengyi Li, Jingwen Leng#, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin#\")]),_vm._v(\", Minyi Guo, Yuhao Zhu \")]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\" AAAI 2022 \")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/2112.08560.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/chandlerguan/blockskim\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" We propose Block-skim, which learns to skim unnecessary context in higher hidden layers to improve and accelerate the Transformer performance. The key idea of Block-Skim is to identify the context that must be further processed and those that could be safely discarded early on during inference. Critically, we find that such information could be sufficiently derived from the self-attention weights inside the Transformer model. We further prune the hidden states corresponding to the unnecessary positions early in lower layers, achieving significant inference-time speedup. To our surprise, we observe that models pruned in this way outperform their full-size counterparts. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/distancelmwenyu.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\" Exploiting Syntactic Structure for Better Language Modeling: A Syntactic Distance Approach \")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Wenyu Du*, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin\")]),_vm._v(\"*, Yikang Shen, Timothy J. O'Donnell, Yoshua Bengio, Yue Zhang# \")]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\" ACL 2020 \")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/2005.05864.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"t https://github.com/wenyudu/SDLM\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" It is commonly believed that knowledge of syntactic structure should improve language modeling. However, effectively and computationally efficiently incorporating syntactic structure into neural language models has been a challenging topic. In this paper, we make use of a multi-task objective, i.e., the models simultaneously predict words as well as ground truth parse trees in a form called \\\"syntactic distances\\\", where information between these two separate objectives shares the same intermediate representation. Experimental results on the Penn Treebank and Chinese Treebank datasets show that when ground truth parse trees are provided as additional training signals, the model is able to achieve lower perplexity and induce trees with better quality. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/distanceparser.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\" Straight to the Tree: Constituency Parsing with Neural Syntactic Distance \")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Yikang Shen*, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin\")]),_vm._v(\"*, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio \")]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\"ACL 2018 \")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/1806.04168.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"http://\"}},[_c('span',[_vm._v(\"slides\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/hantek/distance-parser\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" We propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected, recursively partitioning the input, in a top-down fashion. Compared to traditional shiftreduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize. Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/tree.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\" Neural Language Modeling by Jointly Learning Syntax and Lexicon \")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Yikang Shen, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin\")]),_vm._v(\", Chin-Wei Huang, Aaron Courville \")]),_c('div',{staticClass:\"link-list\"},[_c('span',[_vm._v(\"ICLR 2018\")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/1711.02013.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"http://https://github.com/yikangshen/PRPN\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" We propose a neural language model capable of unsupervised syntactic structure induction. The model leverages the structure information to form better semantic representations and better language modeling. Standard recurrent neural networks are limited by their structure and fail to efficiently use syntactic information. On the other hand, tree-structured recursive networks usually require additional structural supervision at the cost of human expert annotation. In this paper, We propose a novel neural language model, called the Parsing-Reading-Predict Networks (PRPN), that can simultaneously induce the syntactic structure from unannotated sentences and leverage the inferred structure to learn a better language model. In our model, the gradient can be directly back-propagated from the language model loss into the neural parsing network. Experiments show that the proposed model can discover the underlying syntactic structure and achieve state-of-the-art performance on word/character-level language model tasks. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/semlp.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\"A structured self-attentive Sentence Embedding\")]),_c('p',{staticClass:\"author\"},[_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin\")]),_vm._v(\", Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou and Yoshua Bengio \")]),_c('div',{staticClass:\"link-list\"},[_c('span',{staticClass:\"time\"},[_vm._v(\"ICLR 2017\")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/1703.03130.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/hantek/SelfAttentiveSentEmbed\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" We propose a new model for extracting an interpretable sentence embedding by introducing self-attention. Instead of using a vector, we use a 2-D matrix to represent the embedding, with each row of the matrix attending on a different part of the sentence. We also propose a self-attention mechanism and a special regularization term for the model. As a side effect, the embedding comes with an easy way of visualizing what specific parts of the sentence are encoded into the embedding. We evaluate our model on 3 different tasks: author profiling, sentiment classification, and textual entailment. Results show that our model yields a significant performance gain compared to other sentence embedding methods in all of the 3 tasks. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/exp_quant.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\"Neural networks with few multiplications\")]),_c('p',{staticClass:\"author\"},[_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin\")]),_vm._v(\", Matthieu Courbariaux, Roland Memisevic, and Yoshua Bengio \")]),_c('div',{staticClass:\"link-list\"},[_c('span',{staticClass:\"time\"},[_vm._v(\"ICLR 2016 (oral)\")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://arxiv.org/pdf/1510.03009.pdf\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/hantek/BinaryConnect\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" For most deep learning algorithms training is notoriously time consuming. Since most of the computation in training neural networks is typically spent on floating point multiplications, we investigate an approach to training that eliminates the need for most of these. Our method consists of two parts: First we stochastically binarize weights to convert multiplications involved in computing hidden states to sign changes. Second, while back-propagating error derivatives, in addition to binarizing the weights, we quantize the representations at each layer to convert the remaining multiplications into binary shifts. Experimental results across 3 popular datasets (MNIST, CIFAR10, SVHN) show that this approach not only does not hurt classification performance but can result in even better performance than standard stochastic gradient descent training, paving the way to fast, hardwarefriendly training of neural networks. \")])])]),_c('div',{staticClass:\"row\"},[_c('div',{staticClass:\"col-xs-12 col-sm-4\"},[_c('img',{attrs:{\"width\":\"100%\",\"src\":require(\"./../assets/research/hsiclassify.png\"),\"alt\":\"\",\"srcset\":\"\"}})]),_c('div',{staticClass:\"col-xs-12 col-sm-8\"},[_c('h4',[_vm._v(\"Deep learning-based classification of hyperspectral data\")]),_c('p',{staticClass:\"author\"},[_vm._v(\" Yushi Chen, \"),_c('span',{staticStyle:{\"font-weight\":\"500\"}},[_vm._v(\"Zhouhan Lin\")]),_vm._v(\", Xing Zhao, Gang Wang, and Yanfeng Gu \")]),_c('div',{staticClass:\"link-list\"},[_c('span',{staticClass:\"time\"},[_vm._v(\"Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2014\")]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://ieeexplore.ieee.org/document/6844831/?arnumber=6844831\"}},[_c('span',[_vm._v(\"pdf\")])]),_c('span',[_vm._v(\" | \")]),_c('a',{attrs:{\"href\":\"https://github.com/hantek/deeplearn_hsi\"}},[_c('span',[_vm._v(\"codes\")])])]),_c('article',[_vm._v(\" Classification is one of the most popular topics in hyperspectral remote sensing. In the last two decades, a huge number of methods were proposed to deal with the hyperspectral data classification problem. However, most of them do not hierarchically extract deep features. In this paper, the concept of deep learning is introduced into hyperspectral data classification for the first time. First, we verify the eligibility of stacked autoencoders by following classical spectral information-based classification. Second, a new way of classifying with spatial-dominated information is proposed. We then propose a novel deep learning framework to merge the two features, from which we can get the highest classification accuracy. The framework is a hybrid of principle component analysis (PCA), deep learning architecture, and logistic regression. Specifically, as a deep learning architecture, stacked autoencoders are aimed to get useful high-level features. Experimental results with widely-used hyperspectral data indicate that classifiers built in this deep learning-based framework provide competitive performance. In addition, the proposed joint spectral–spatial deep neural network opens a new window for future research, showcasing the deep learning-based methods’ huge potential for accurate hyperspectral data classification. \")])])])])])])])\n}]\n\nexport { render, staticRenderFns }","// Research页面相关数据\n// 按规则修改和添加内容即可，请不要更改数据结构以及变量命名\nexport const researchData = {\n    // research列表\n    researchList: [\n        // 第一个research\n        {\n            // 名称（标题）\n            name: \"Straight to the Tree: Constituency Parsing with Neural Syntactic Distance\",\n            // 图片地址\n            img: \"./static/research/distanceparser.png\",\n            // 详情（若为链接类型，则加入link: 'http://'）\n            list: [{\n                    name: 'ACL 2018'\n                },\n                {\n                    name: 'pdf',\n                    link: 'http:'\n                },\n                {\n                    name: 'slides',\n                    link: 'http:'\n                },\n                {\n                    name: 'codes',\n                    link: 'http:'\n                },\n            ],\n            author: \"Yikang Shen*, Zhouhan Lin*, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio\",\n\n            abstract: \"We propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected, recursively partitioning the input, in a top-down fashion. Compared to traditional shiftreduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize. Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.\",\n        },\n        {\n            name: \"Straight to the Tree: Constituency Parsing with Neural Syntactic Distance\",\n            img: \"./static/research/distanceparser.png\",\n            list: [{\n                    name: 'ACL 2018'\n                },\n                {\n                    name: 'pdf',\n                    link: 'http:'\n                },\n                {\n                    name: 'slides',\n                    link: 'http:'\n                },\n                {\n                    name: 'codes',\n                    link: 'http:'\n                },\n            ],\n            author: \"Yikang Shen*, Zhouhan Lin*, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio\",\n            abstract: \"We propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected, recursively partitioning the input, in a top-down fashion. Compared to traditional shiftreduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize. Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.\",\n        },\n        {\n            name: \"Straight to the Tree: Constituency Parsing with Neural Syntactic Distance\",\n            img: \"./static/research/distanceparser.png\",\n            list: [{\n                    name: 'ACL 2018'\n                },\n                {\n                    name: 'pdf',\n                    link: 'http:'\n                },\n                {\n                    name: 'slides',\n                    link: 'http:'\n                },\n                {\n                    name: 'codes',\n                    link: 'http:'\n                },\n            ],\n            author: \"Yikang Shen*, Zhouhan Lin*, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio\",\n            abstract: \"We propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected, recursively partitioning the input, in a top-down fashion. Compared to traditional shiftreduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize. Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.\",\n        },\n        {\n            name: \"Straight to the Tree: Constituency Parsing with Neural Syntactic Distance\",\n            img: \"./static/research/distanceparser.png\",\n            list: [{\n                    name: 'ACL 2018'\n                },\n                {\n                    name: 'pdf',\n                    link: 'http:'\n                },\n                {\n                    name: 'slides',\n                    link: 'http:'\n                },\n                {\n                    name: 'codes',\n                    link: 'http:'\n                },\n            ],\n            author: \"Yikang Shen*, Zhouhan Lin*, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio\",\n            abstract: \"We propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected, recursively partitioning the input, in a top-down fashion. Compared to traditional shiftreduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize. Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.\",\n        },\n        {\n            name: \"Straight to the Tree: Constituency Parsing with Neural Syntactic Distance\",\n            img: \"./static/research/distanceparser.png\",\n            list: [{\n                    name: 'ACL 2018'\n                },\n                {\n                    name: 'pdf',\n                    link: 'http:'\n                },\n                {\n                    name: 'slides',\n                    link: 'http:'\n                },\n                {\n                    name: 'codes',\n                    link: 'http:'\n                },\n            ],\n            author: \"Yikang Shen*, Zhouhan Lin*, Athul Paul Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio\",\n            abstract: \"We propose a novel constituency parsing scheme. The model predicts a vector of real-valued scalars, named syntactic distances, for each split position in the input sentence. The syntactic distances specify the order in which the split points will be selected, recursively partitioning the input, in a top-down fashion. Compared to traditional shiftreduce parsing schemes, our approach is free from the potential problem of compounding errors, while being faster and easier to parallelize. Our model achieves competitive performance amongst single model, discriminative parsers in the PTB dataset and outperforms previous models in the CTB dataset.\",\n        },\n    ],\n}","<template>\n  <div class=\"research-container\">\n    <div class=\"research-content\">\n      <section id=\"publications\" class=\"section-item\">\n        <section class=\"research-section\">\n          <!-- research 1 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/fouriertransformer.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>\n                Fourier Transformer: Fast Long Range Modeling by Removing\n                Sequence Redundancy with FFT Operator\n              </h4>\n              <p class=\"author\">\n                Ziwei He, Meng Yang, Minwei Feng, Jingcheng Yin, Xinbing Wang,\n                Jingwen Leng, <span style=\"font-weight: 500\">Zhouhan Lin#</span>\n              </p>\n              <div class=\"link-list\">\n                <span>ACL 2023 (Findings) </span>\n                <span> | </span>\n                <a href=\"https://aclanthology.org/2023.findings-acl.570.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"https://github.com/LUMIA-Group/FourierTransformer\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                We propose Fourier Transformer, a simple yet effective approach\n                by layer-wise progressively removing sequence redundancies in\n                hidden states using the ready-made Fast Fourier Transform (FFT)\n                operator to perform Discrete Cosine Transformation (DCT).\n                Fourier Transformer is able to significantly reduce\n                computational costs while retaining the ability to inherit from\n                various large pretrained models. SOTA performances among all\n                transformer-based models on the LRA benchmark with significant\n                improvement in both speed and space. For generative seq-to-seq\n                tasks including CNN/DailyMail and ELI5, by inheriting the BART\n                weights our model outperforms the standard BART and other\n                efficient models.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 1 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/lotinsts.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>\n                Text Classification In The Wild: A Large-Scale Long-Tailed Name\n                Normalization Dataset\n              </h4>\n              <p class=\"author\">\n                Jiexing Qi, Shuhao Li, Zhixin Guo, Yusheng Huang, Chenghu Zhou,\n                Weinan Zhang, Xinbing Wang,\n                <span style=\"font-weight: 500\">Zhouhan Lin#</span>\n              </p>\n              <div class=\"link-list\">\n                <span>ICASSP 2023 </span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/2302.09509.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"https://github.com/LUMIA-Group/LoT-insts\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                In this work, we first collect a large-scale institution name\n                normalization dataset LoT-insts, which contains over 25k classes\n                that exhibit a naturally long-tailed distribution. In order to\n                isolate the few-shot and zero-shot learning scenarios from the\n                massive many-shot classes, we construct our test set from four\n                different subsets: many-, medium-, and few-shot sets, as well as\n                a zero-shot open set. We believe it provides an important and\n                different scenario to study this problem.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 1 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/orderedgnn.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>\n                Ordered GNN: Ordering Message Passing to Deal with Heterophily\n                and Over-smoothing\n              </h4>\n              <p class=\"author\">\n                Yunchong Song, Chenghu Zhou, Xinbing Wang,\n                <span style=\"font-weight: 500\">Zhouhan Lin#</span>\n              </p>\n              <div class=\"link-list\">\n                <span>ICLR 2023 </span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/2302.01524.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"https://github.com/LUMIA-Group/OrderedGNN\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                In this work, we propose to tackle both heterophily and\n                over-smoothing problems by an ordered message passing mecanism,\n                with specific blocks of neurons in a node embedding targeted for\n                messages passed from neighboring nodes that are located within\n                specific hops. This is achieved by aligning the hierarchy of the\n                rooted-tree of a central node with the ordered neurons in its\n                node representation. SOTA performance in both homophily and\n                heterophily settings without any targeted design, robust to a\n                wide number of layers, and explainable.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 1 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/rasat.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>\n                RASAT: Integrating Relational Structures into Pretrained Seq2Seq\n                Model for Text-to-SQL\n              </h4>\n              <p class=\"author\">\n                Jiexing Qi, Jingyao Tang, Ziwei He, Xiangpeng Wan, Yu Cheng,\n                Chenghu Zhou, Xinbing Wang, Quanshi Zhang,\n                <span style=\"font-weight: 500\">Zhouhan Lin#</span>\n              </p>\n              <div class=\"link-list\">\n                <span>EMNLP 2022 </span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/2205.06983.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"https://github.com/LUMIA-Group/rasat\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                Relational structures such as schema linking and schema encoding\n                have been validated as a key component to qualitatively\n                translating natural language into SQL queries. We propose RASAT:\n                a Transformer seq2seq architecture augmented with relation-aware\n                self-attention that could leverage a variety of relational\n                structures while still being able to inherit the pretrained\n                parameters from the T5 model effectively. Our model can\n                incorporate almost all types of existing relations in the\n                literature. Experimental results on three widely used\n                text-to-SQL datasets, covering both single-turn and multi-turn\n                scenarios, have shown that RASAT could achieve state-of-the-art\n                results across all three benchmarks (75.5% EX on Spider, 52.6%\n                IEX on SParC, and 37.4% IEX on CoSQL).\n              </article>\n            </div>\n          </div>\n\n          <!-- research 1 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/distancetransformer.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>\n                Syntax-guided Localized Self-attention by Constituency Syntactic\n                Distance\n              </h4>\n              <p class=\"author\">\n                Shengyuan Hou*, Jushi Kai*, Haotian Xue*, Bingyu Zhu, Bo Yuan,\n                Longtao Huang, Xinbing Wang,\n                <span style=\"font-weight: 500\">Zhouhan Lin#</span>\n              </p>\n              <div class=\"link-list\">\n                <span>EMNLP 2022 (Findings) </span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/2210.11759.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"https://github.com/LUMIA-Group/distance_transformer\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                We propose a syntax-guided localized self-attention for\n                Transformer that allows directly incorporating grammar\n                structures from an external constituency parser. It prohibits\n                the attention mechanism from overweight the grammatically\n                distant tokens over close ones.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 1 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/avsrpan.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>\n                Leveraging Unimodal Self-Supervised Learning for Multimodal\n                Audio-Visual Speech Recognition\n              </h4>\n              <p class=\"author\">\n                Xichen Pan, Peiyu Chen, Yichen Gong, Helong Zhou, Xinbing Wang,\n                <span style=\"font-weight: 500\">Zhouhan Lin#</span>\n              </p>\n              <div class=\"link-list\">\n                <span> ACL 2022 </span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/2203.07996.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a\n                  href=\"https://github.com/LUMIA-Group/Leveraging-Self-Supervised-Learning-for-AVSR\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                In this work, we successfully leverage unimodal self-supervised\n                learning to promote the multimodal AVSR. In particular, audio\n                and visual front-ends are trained on large-scale unimodal\n                datasets, and then we integrate components of both front-ends\n                into a larger multimodal framework that learns to recognize\n                parallel audio-visual data into characters through a combination\n                of CTC and seq2seq decoding. We show that both components\n                inherited from unimodal self-supervised learning cooperate well,\n                resulting in the multimodal framework yielding competitive\n                results through fine-tuning. Even without an external language\n                model, our proposed model raises the SOTA performances on the\n                widely accepted Lip Reading Sentences 2 (LRS2) dataset by a\n                large margin, with a relative improvement of 30%.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 1 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/blockskim.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>Block-Skim: Efficient Question Answering for Transformer</h4>\n              <p class=\"author\">\n                Yue Guan, Zhengyi Li, Jingwen Leng#,\n                <span style=\"font-weight: 500\">Zhouhan Lin#</span>, Minyi Guo,\n                Yuhao Zhu\n              </p>\n              <div class=\"link-list\">\n                <span> AAAI 2022 </span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/2112.08560.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"https://github.com/chandlerguan/blockskim\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                We propose Block-skim, which learns to skim unnecessary context\n                in higher hidden layers to improve and accelerate the\n                Transformer performance. The key idea of Block-Skim is to\n                identify the context that must be further processed and those\n                that could be safely discarded early on during inference.\n                Critically, we find that such information could be sufficiently\n                derived from the self-attention weights inside the Transformer\n                model. We further prune the hidden states corresponding to the\n                unnecessary positions early in lower layers, achieving\n                significant inference-time speedup. To our surprise, we observe\n                that models pruned in this way outperform their full-size\n                counterparts.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 1 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/distancelmwenyu.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>\n                Exploiting Syntactic Structure for Better Language Modeling: A\n                Syntactic Distance Approach\n              </h4>\n              <p class=\"author\">\n                Wenyu Du*, <span style=\"font-weight: 500\">Zhouhan Lin</span>*,\n                Yikang Shen, Timothy J. O'Donnell, Yoshua Bengio, Yue Zhang#\n              </p>\n              <div class=\"link-list\">\n                <span> ACL 2020 </span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/2005.05864.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"t https://github.com/wenyudu/SDLM\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                It is commonly believed that knowledge of syntactic structure\n                should improve language modeling. However, effectively and\n                computationally efficiently incorporating syntactic structure\n                into neural language models has been a challenging topic. In\n                this paper, we make use of a multi-task objective, i.e., the\n                models simultaneously predict words as well as ground truth\n                parse trees in a form called \"syntactic distances\", where\n                information between these two separate objectives shares the\n                same intermediate representation. Experimental results on the\n                Penn Treebank and Chinese Treebank datasets show that when\n                ground truth parse trees are provided as additional training\n                signals, the model is able to achieve lower perplexity and\n                induce trees with better quality.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 1 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/distanceparser.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>\n                Straight to the Tree: Constituency Parsing with Neural Syntactic\n                Distance\n              </h4>\n              <p class=\"author\">\n                Yikang Shen*,\n                <span style=\"font-weight: 500\">Zhouhan Lin</span>*, Athul Paul\n                Jacob, Alessandro Sordoni, Aaron Courville, Yoshua Bengio\n              </p>\n              <div class=\"link-list\">\n                <span>ACL 2018 </span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/1806.04168.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"http://\"><span>slides</span></a>\n                <span> | </span>\n                <a href=\"https://github.com/hantek/distance-parser\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                We propose a novel constituency parsing scheme. The model\n                predicts a vector of real-valued scalars, named syntactic\n                distances, for each split position in the input sentence. The\n                syntactic distances specify the order in which the split points\n                will be selected, recursively partitioning the input, in a\n                top-down fashion. Compared to traditional shiftreduce parsing\n                schemes, our approach is free from the potential problem of\n                compounding errors, while being faster and easier to\n                parallelize. Our model achieves competitive performance amongst\n                single model, discriminative parsers in the PTB dataset and\n                outperforms previous models in the CTB dataset.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 3 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/tree.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>\n                Neural Language Modeling by Jointly Learning Syntax and Lexicon\n              </h4>\n              <p class=\"author\">\n                Yikang Shen, <span style=\"font-weight: 500\">Zhouhan Lin</span>,\n                Chin-Wei Huang, Aaron Courville\n              </p>\n              <div class=\"link-list\">\n                <span>ICLR 2018</span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/1711.02013.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"http://https://github.com/yikangshen/PRPN\"\n                  ><span>codes</span></a\n                >\n                <!-- <span> | </span> -->\n                <!-- <a href=\"\"><span>poster</span></a> -->\n              </div>\n              <article>\n                We propose a neural language model capable of unsupervised\n                syntactic structure induction. The model leverages the structure\n                information to form better semantic representations and better\n                language modeling. Standard recurrent neural networks are\n                limited by their structure and fail to efficiently use syntactic\n                information. On the other hand, tree-structured recursive\n                networks usually require additional structural supervision at\n                the cost of human expert annotation. In this paper, We propose a\n                novel neural language model, called the Parsing-Reading-Predict\n                Networks (PRPN), that can simultaneously induce the syntactic\n                structure from unannotated sentences and leverage the inferred\n                structure to learn a better language model. In our model, the\n                gradient can be directly back-propagated from the language model\n                loss into the neural parsing network. Experiments show that the\n                proposed model can discover the underlying syntactic structure\n                and achieve state-of-the-art performance on word/character-level\n                language model tasks.\n              </article>\n            </div>\n          </div>\n          <!-- research 4 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/semlp.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>A structured self-attentive Sentence Embedding</h4>\n              <p class=\"author\">\n                <span style=\"font-weight: 500\">Zhouhan Lin</span>, Minwei Feng,\n                Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou and\n                Yoshua Bengio\n              </p>\n              <div class=\"link-list\">\n                <span class=\"time\">ICLR 2017</span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/1703.03130.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"https://github.com/hantek/SelfAttentiveSentEmbed\"\n                  ><span>codes</span></a\n                >\n                <!-- <span> | </span> -->\n                <!-- <a href=\"\"><span>poster</span></a> -->\n              </div>\n              <article>\n                We propose a new model for extracting an interpretable sentence\n                embedding by introducing self-attention. Instead of using a\n                vector, we use a 2-D matrix to represent the embedding, with\n                each row of the matrix attending on a different part of the\n                sentence. We also propose a self-attention mechanism and a\n                special regularization term for the model. As a side effect, the\n                embedding comes with an easy way of visualizing what specific\n                parts of the sentence are encoded into the embedding. We\n                evaluate our model on 3 different tasks: author profiling,\n                sentiment classification, and textual entailment. Results show\n                that our model yields a significant performance gain compared to\n                other sentence embedding methods in all of the 3 tasks.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 5 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/exp_quant.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>Neural networks with few multiplications</h4>\n              <p class=\"author\">\n                <span style=\"font-weight: 500\">Zhouhan Lin</span>, Matthieu\n                Courbariaux, Roland Memisevic, and Yoshua Bengio\n              </p>\n              <div class=\"link-list\">\n                <span class=\"time\">ICLR 2016 (oral)</span>\n                <span> | </span>\n                <a href=\"https://arxiv.org/pdf/1510.03009.pdf\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"https://github.com/hantek/BinaryConnect\"\n                  ><span>codes</span></a\n                >\n                <!-- <span> | </span> -->\n                <!-- <a href=\"\"><span>poster</span></a> -->\n              </div>\n              <article>\n                For most deep learning algorithms training is notoriously time\n                consuming. Since most of the computation in training neural\n                networks is typically spent on floating point multiplications,\n                we investigate an approach to training that eliminates the need\n                for most of these. Our method consists of two parts: First we\n                stochastically binarize weights to convert multiplications\n                involved in computing hidden states to sign changes. Second,\n                while back-propagating error derivatives, in addition to\n                binarizing the weights, we quantize the representations at each\n                layer to convert the remaining multiplications into binary\n                shifts. Experimental results across 3 popular datasets (MNIST,\n                CIFAR10, SVHN) show that this approach not only does not hurt\n                classification performance but can result in even better\n                performance than standard stochastic gradient descent training,\n                paving the way to fast, hardwarefriendly training of neural\n                networks.\n              </article>\n            </div>\n          </div>\n\n          <!-- research 8 -->\n          <div class=\"row\">\n            <div class=\"col-xs-12 col-sm-4\">\n              <img\n                width=\"100%\"\n                src=\"./../assets/research/hsiclassify.png\"\n                alt=\"\"\n                srcset=\"\"\n              />\n            </div>\n            <div class=\"col-xs-12 col-sm-8\">\n              <h4>Deep learning-based classification of hyperspectral data</h4>\n              <p class=\"author\">\n                Yushi Chen, <span style=\"font-weight: 500\">Zhouhan Lin</span>,\n                Xing Zhao, Gang Wang, and Yanfeng Gu\n              </p>\n              <div class=\"link-list\">\n                <span class=\"time\"\n                  >Journal of Selected Topics in Applied Earth Observations and\n                  Remote Sensing, 2014</span\n                >\n                <span> | </span>\n                <a\n                  href=\"https://ieeexplore.ieee.org/document/6844831/?arnumber=6844831\"\n                  ><span>pdf</span></a\n                >\n                <span> | </span>\n                <a href=\"https://github.com/hantek/deeplearn_hsi\"\n                  ><span>codes</span></a\n                >\n              </div>\n              <article>\n                Classification is one of the most popular topics in\n                hyperspectral remote sensing. In the last two decades, a huge\n                number of methods were proposed to deal with the hyperspectral\n                data classification problem. However, most of them do not\n                hierarchically extract deep features. In this paper, the concept\n                of deep learning is introduced into hyperspectral data\n                classification for the first time. First, we verify the\n                eligibility of stacked autoencoders by following classical\n                spectral information-based classification. Second, a new way of\n                classifying with spatial-dominated information is proposed. We\n                then propose a novel deep learning framework to merge the two\n                features, from which we can get the highest classification\n                accuracy. The framework is a hybrid of principle component\n                analysis (PCA), deep learning architecture, and logistic\n                regression. Specifically, as a deep learning architecture,\n                stacked autoencoders are aimed to get useful high-level\n                features. Experimental results with widely-used hyperspectral\n                data indicate that classifiers built in this deep learning-based\n                framework provide competitive performance. In addition, the\n                proposed joint spectral–spatial deep neural network opens a new\n                window for future research, showcasing the deep learning-based\n                methods’ huge potential for accurate hyperspectral data\n                classification.\n              </article>\n            </div>\n          </div>\n\n          <!-- 在这里新建一个新的research，可复制一段research代码，从class=row开始, 格式如下 -->\n          <!-- <div class=\"row\">……</div> -->\n        </section>\n      </section>\n    </div>\n  </div>\n</template>\n<script>\nimport { researchData } from \"@/data/research\";\nexport default {\n  data() {\n    return {\n      researchData: researchData,\n    };\n  },\n};\n</script>\n<style lang=\"less\" scoped>\n.research-content {\n  max-width: 1280px;\n  margin: auto;\n  padding-top: 40px;\n  text-align: left;\n  .research-section {\n    .row {\n      margin-top: 24px !important;\n      display: flex;\n      flex-wrap: wrap;\n      padding: 32px 0;\n      border-bottom: 1px solid #004d4c;\n      &:last-child {\n        border: none;\n      }\n    }\n    .author {\n      margin-bottom: 8px;\n      text-align: justify;\n    }\n    .link-list {\n      margin-bottom: 8px;\n    }\n    article {\n      font-size: 15px;\n      text-align: justify;\n      line-height: 24px;\n    }\n    .col-sm-4 {\n        flex: 0 0 auto;\n        width: 33.33333333%;\n        \n    }\n    .col-sm-8 {\n      flex: 0 0 auto;\n      width: 66.66666667%;\n      padding-left: 24px;\n      box-sizing: border-box;\n    }\n    h4 {\n        font-size: 19px;\n        line-height: 28px;\n        font-weight: 600;\n        margin-bottom: 16px;\n    }\n  }\n}\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Research.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Research.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Research.vue?vue&type=template&id=1d1203e8&scoped=true&\"\nimport script from \"./Research.vue?vue&type=script&lang=js&\"\nexport * from \"./Research.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Research.vue?vue&type=style&index=0&id=1d1203e8&prod&lang=less&scoped=true&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"1d1203e8\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"news-container\"},[_c('div',{staticClass:\"news-content\"},[_c('ul',_vm._l((_vm.newsData.newsList),function(item,index){return _c('li',{key:index},[_vm._v(\" \"+_vm._s(item)+\" \")])}),0)])])\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","// News页面相关数据\n// 按规则修改和添加内容即可，请不要更改数据结构以及变量命名\nexport const newsData = {\n    newsList: [\n        // 每一条新闻\n        \"2023/01: Two papers are accepted at ICLR 2023.\",\n        \"2023/01: Two papers are accepted at ICLR 2023.\",\n        \"2023/01: Two papers are accepted at ICLR 2023.\",\n        \"2023/01: Two papers are accepted at ICLR 2023.\",\n    ],\n}","<template>\n  <div class=\"news-container\">\n    <div class=\"news-content\">\n      <ul>\n        <li v-for=\"(item, index) in newsData.newsList\" :key=\"index\">\n          {{ item }}\n        </li>\n      </ul>\n    </div>\n  </div>\n</template>\n<script>\nimport { newsData } from '@/data/news'\nexport default {\n  data() {\n    return {\n      newsData: newsData\n    };\n  },\n};\n</script>\n<style lang=\"less\" scoped>\n.news-content {\n  max-width: 1280px;\n  margin: auto;\n  padding-top: 80px;\n  text-align: left;\n  li {\n    list-style: circle;\n    line-height: 36px;\n  }\n}\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./News.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./News.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./News.vue?vue&type=template&id=812e5dda&scoped=true&\"\nimport script from \"./News.vue?vue&type=script&lang=js&\"\nexport * from \"./News.vue?vue&type=script&lang=js&\"\nimport style0 from \"./News.vue?vue&type=style&index=0&id=812e5dda&prod&lang=less&scoped=true&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"812e5dda\",\n  null\n  \n)\n\nexport default component.exports","var render = function render(){var _vm=this,_c=_vm._self._c;return _c('div',{staticClass:\"contact-container\"},[_c('div',{staticClass:\"contact-content\"},_vm._l((_vm.contactData.contactList),function(item,index){return _c('section',{key:index,staticClass:\"contact-list\"},[_c('h2',[_vm._v(_vm._s(item.header))]),_vm._l((item.list),function(l,lindex){return _c('p',{key:lindex + 'l'},[_vm._v(_vm._s(l))])})],2)}),0)])\n}\nvar staticRenderFns = []\n\nexport { render, staticRenderFns }","// Contact页面相关数据\n// 按规则修改和添加内容即可，请不要更改数据结构以及变量命名\nexport const contactData = {\n    contactList: [\n        // 第一个section\n        {\n            header: \"Header 1\",\n            list: [\n                // 第一段\n                \"This is line 1\",\n                // 第二段\n                \"This is a line, This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,This is a line,\",\n            ],\n        },\n        // 第二个section\n        {\n            header: \"Header 2\",\n            list: [\"This is line 1\"],\n        },\n        {\n            header: \"Header 3\",\n            list: [\"This is line 1\"],\n        },\n    ],\n}","<template>\n  <div class=\"contact-container\">\n    <div class=\"contact-content\">\n      <section\n        class=\"contact-list\"\n        v-for=\"(item, index) in contactData.contactList\"\n        :key=\"index\"\n      >\n        <h2>{{ item.header }}</h2>\n        <p v-for=\"(l, lindex) in item.list\" :key=\"lindex + 'l'\">{{ l }}</p>\n      </section>\n    </div>\n  </div>\n</template>\n<script>\nimport { contactData } from '@/data/contact'\nexport default {\n  data() {\n    return {\n      contactData: contactData\n    };\n  },\n};\n</script>\n<style lang=\"less\" scoped>\n.contact-content {\n  max-width: 1280px;\n  margin: auto;\n  padding-top: 48px;\n  text-align: left;\n  .contact-list {\n    padding: 32px 0;\n    border-bottom: 1px solid #004d4c;\n    &:last-child {\n        border: none;\n    }\n  }\n  p {\n    line-height: 20px;\n    margin-bottom: 12px;\n    text-align: justify;\n  }\n  h2 {\n    font-size: 22px;\n    font-weight: 600;\n    margin-bottom: 18px;\n    color: #004d4c;\n  }\n}\n</style>","import mod from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Contact.vue?vue&type=script&lang=js&\"; export default mod; export * from \"-!../../node_modules/thread-loader/dist/cjs.js!../../node_modules/babel-loader/lib/index.js??clonedRuleSet-40.use[1]!../../node_modules/@vue/vue-loader-v15/lib/index.js??vue-loader-options!./Contact.vue?vue&type=script&lang=js&\"","import { render, staticRenderFns } from \"./Contact.vue?vue&type=template&id=2be34353&scoped=true&\"\nimport script from \"./Contact.vue?vue&type=script&lang=js&\"\nexport * from \"./Contact.vue?vue&type=script&lang=js&\"\nimport style0 from \"./Contact.vue?vue&type=style&index=0&id=2be34353&prod&lang=less&scoped=true&\"\n\n\n/* normalize component */\nimport normalizer from \"!../../node_modules/@vue/vue-loader-v15/lib/runtime/componentNormalizer.js\"\nvar component = normalizer(\n  script,\n  render,\n  staticRenderFns,\n  false,\n  null,\n  \"2be34353\",\n  null\n  \n)\n\nexport default component.exports","import Vue from \"vue\";\nimport VueRouter from \"vue-router\";\nimport HomeView from \"../views/HomeView.vue\";\nimport People from \"../views/People.vue\";\nimport Research from \"../views/Research.vue\";\nimport News from \"../views/News.vue\";\nimport Contact from \"../views/Contact.vue\";\n\nVue.use(VueRouter);\n\nconst routes = [{\n        path: \"/\",\n        name: \"home\",\n        component: HomeView,\n    }, {\n        path: \"/people\",\n        name: \"people\",\n        component: People,\n    },\n    {\n        path: \"/research\",\n        name: \"research\",\n        component: Research,\n    },\n    {\n        path: \"/news\",\n        name: \"news\",\n        component: News,\n    },\n    {\n        path: \"/contact\",\n        name: \"contact\",\n        component: Contact,\n    },\n];\n\nconst router = new VueRouter({\n    // mode: \"history\",\n    base: process.env.BASE_URL,\n    routes,\n});\n\nexport default router;","import Vue from \"vue\";\nimport Vuex from \"vuex\";\n\nVue.use(Vuex);\n\nexport default new Vuex.Store({\n  state: {},\n  getters: {},\n  mutations: {},\n  actions: {},\n  modules: {},\n});\n","import Vue from \"vue\";\nimport App from \"./App.vue\";\nimport router from \"./router\";\nimport store from \"./store\";\n\nimport ElementUI from 'element-ui';\nimport 'element-ui/lib/theme-chalk/index.css';\n\nVue.use(ElementUI);\nVue.config.productionTip = false;\n\nnew Vue({\n    router,\n    store,\n    render: (h) => h(App),\n}).$mount(\"#app\");","// The module cache\nvar __webpack_module_cache__ = {};\n\n// The require function\nfunction __webpack_require__(moduleId) {\n\t// Check if module is in cache\n\tvar cachedModule = __webpack_module_cache__[moduleId];\n\tif (cachedModule !== undefined) {\n\t\treturn cachedModule.exports;\n\t}\n\t// Create a new module (and put it into the cache)\n\tvar module = __webpack_module_cache__[moduleId] = {\n\t\tid: moduleId,\n\t\tloaded: false,\n\t\texports: {}\n\t};\n\n\t// Execute the module function\n\t__webpack_modules__[moduleId].call(module.exports, module, module.exports, __webpack_require__);\n\n\t// Flag the module as loaded\n\tmodule.loaded = true;\n\n\t// Return the exports of the module\n\treturn module.exports;\n}\n\n// expose the modules object (__webpack_modules__)\n__webpack_require__.m = __webpack_modules__;\n\n","__webpack_require__.amdO = {};","var deferred = [];\n__webpack_require__.O = function(result, chunkIds, fn, priority) {\n\tif(chunkIds) {\n\t\tpriority = priority || 0;\n\t\tfor(var i = deferred.length; i > 0 && deferred[i - 1][2] > priority; i--) deferred[i] = deferred[i - 1];\n\t\tdeferred[i] = [chunkIds, fn, priority];\n\t\treturn;\n\t}\n\tvar notFulfilled = Infinity;\n\tfor (var i = 0; i < deferred.length; i++) {\n\t\tvar chunkIds = deferred[i][0];\n\t\tvar fn = deferred[i][1];\n\t\tvar priority = deferred[i][2];\n\t\tvar fulfilled = true;\n\t\tfor (var j = 0; j < chunkIds.length; j++) {\n\t\t\tif ((priority & 1 === 0 || notFulfilled >= priority) && Object.keys(__webpack_require__.O).every(function(key) { return __webpack_require__.O[key](chunkIds[j]); })) {\n\t\t\t\tchunkIds.splice(j--, 1);\n\t\t\t} else {\n\t\t\t\tfulfilled = false;\n\t\t\t\tif(priority < notFulfilled) notFulfilled = priority;\n\t\t\t}\n\t\t}\n\t\tif(fulfilled) {\n\t\t\tdeferred.splice(i--, 1)\n\t\t\tvar r = fn();\n\t\t\tif (r !== undefined) result = r;\n\t\t}\n\t}\n\treturn result;\n};","// getDefaultExport function for compatibility with non-harmony modules\n__webpack_require__.n = function(module) {\n\tvar getter = module && module.__esModule ?\n\t\tfunction() { return module['default']; } :\n\t\tfunction() { return module; };\n\t__webpack_require__.d(getter, { a: getter });\n\treturn getter;\n};","// define getter functions for harmony exports\n__webpack_require__.d = function(exports, definition) {\n\tfor(var key in definition) {\n\t\tif(__webpack_require__.o(definition, key) && !__webpack_require__.o(exports, key)) {\n\t\t\tObject.defineProperty(exports, key, { enumerable: true, get: definition[key] });\n\t\t}\n\t}\n};","__webpack_require__.g = (function() {\n\tif (typeof globalThis === 'object') return globalThis;\n\ttry {\n\t\treturn this || new Function('return this')();\n\t} catch (e) {\n\t\tif (typeof window === 'object') return window;\n\t}\n})();","__webpack_require__.o = function(obj, prop) { return Object.prototype.hasOwnProperty.call(obj, prop); }","// define __esModule on exports\n__webpack_require__.r = function(exports) {\n\tif(typeof Symbol !== 'undefined' && Symbol.toStringTag) {\n\t\tObject.defineProperty(exports, Symbol.toStringTag, { value: 'Module' });\n\t}\n\tObject.defineProperty(exports, '__esModule', { value: true });\n};","__webpack_require__.nmd = function(module) {\n\tmodule.paths = [];\n\tif (!module.children) module.children = [];\n\treturn module;\n};","__webpack_require__.p = \"/\";","// no baseURI\n\n// object to store loaded and loading chunks\n// undefined = chunk not loaded, null = chunk preloaded/prefetched\n// [resolve, reject, Promise] = chunk loading, 0 = chunk loaded\nvar installedChunks = {\n\t143: 0\n};\n\n// no chunk on demand loading\n\n// no prefetching\n\n// no preloaded\n\n// no HMR\n\n// no HMR manifest\n\n__webpack_require__.O.j = function(chunkId) { return installedChunks[chunkId] === 0; };\n\n// install a JSONP callback for chunk loading\nvar webpackJsonpCallback = function(parentChunkLoadingFunction, data) {\n\tvar chunkIds = data[0];\n\tvar moreModules = data[1];\n\tvar runtime = data[2];\n\t// add \"moreModules\" to the modules object,\n\t// then flag all \"chunkIds\" as loaded and fire callback\n\tvar moduleId, chunkId, i = 0;\n\tif(chunkIds.some(function(id) { return installedChunks[id] !== 0; })) {\n\t\tfor(moduleId in moreModules) {\n\t\t\tif(__webpack_require__.o(moreModules, moduleId)) {\n\t\t\t\t__webpack_require__.m[moduleId] = moreModules[moduleId];\n\t\t\t}\n\t\t}\n\t\tif(runtime) var result = runtime(__webpack_require__);\n\t}\n\tif(parentChunkLoadingFunction) parentChunkLoadingFunction(data);\n\tfor(;i < chunkIds.length; i++) {\n\t\tchunkId = chunkIds[i];\n\t\tif(__webpack_require__.o(installedChunks, chunkId) && installedChunks[chunkId]) {\n\t\t\tinstalledChunks[chunkId][0]();\n\t\t}\n\t\tinstalledChunks[chunkId] = 0;\n\t}\n\treturn __webpack_require__.O(result);\n}\n\nvar chunkLoadingGlobal = self[\"webpackChunkLUMIA_Group\"] = self[\"webpackChunkLUMIA_Group\"] || [];\nchunkLoadingGlobal.forEach(webpackJsonpCallback.bind(null, 0));\nchunkLoadingGlobal.push = webpackJsonpCallback.bind(null, chunkLoadingGlobal.push.bind(chunkLoadingGlobal));","// startup\n// Load entry module and return exports\n// This entry module depends on other loaded chunks and execution need to be delayed\nvar __webpack_exports__ = __webpack_require__.O(undefined, [998], function() { return __webpack_require__(3921); })\n__webpack_exports__ = __webpack_require__.O(__webpack_exports__);\n"],"names":["render","_vm","this","_c","_self","attrs","staticRenderFns","staticClass","on","$event","clickHeader","value","headerData","logo","_v","_s","name","_l","headerList","item","key","class","active","activeHeader","label","type","data","mounted","$route","methods","window","open","$router","push","component","components","appHeader","homeData","desc","projectList","id","img","intro","model","activeNames","callback","$$v","expression","peopleData","studentsList","index","pic","grade","teachersList","homepage","_m","require","staticStyle","researchData","researchList","list","link","author","abstract","newsData","newsList","contactData","contactList","header","l","lindex","Vue","use","VueRouter","routes","path","HomeView","People","Research","News","Contact","router","base","process","Vuex","state","getters","mutations","actions","modules","ElementUI","config","productionTip","store","h","App","$mount","__webpack_module_cache__","__webpack_require__","moduleId","cachedModule","undefined","exports","module","loaded","__webpack_modules__","call","m","amdO","deferred","O","result","chunkIds","fn","priority","notFulfilled","Infinity","i","length","fulfilled","j","Object","keys","every","splice","r","n","getter","__esModule","d","a","definition","o","defineProperty","enumerable","get","g","globalThis","Function","e","obj","prop","prototype","hasOwnProperty","Symbol","toStringTag","nmd","paths","children","p","installedChunks","chunkId","webpackJsonpCallback","parentChunkLoadingFunction","moreModules","runtime","some","chunkLoadingGlobal","self","forEach","bind","__webpack_exports__"],"sourceRoot":""}